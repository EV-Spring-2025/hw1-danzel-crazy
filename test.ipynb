{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import wandb\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from torch_3dgs.data import read_data\n",
    "from torch_3dgs.trainer import Trainer\n",
    "from torch_3dgs.model import GaussianModel\n",
    "from torch_3dgs.point import get_point_clouds\n",
    "from torch_3dgs.utils import dict_to_device, visualize_points_plotly\n",
    "import random\n",
    "from typing import BinaryIO, Dict, List, Optional, Union\n",
    "\n",
    "config = OmegaConf.load(\"config.yaml\")\n",
    "os.makedirs(config.output_folder, exist_ok=True)\n",
    "device = torch.device(config.device)\n",
    "\n",
    "data = read_data(config.data_folder, resize_scale=config.resize_scale)\n",
    "# data = dict_to_device(data, device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_3dgs.camera import extract_camera_params\n",
    "from torch_3dgs.point import PointCloud\n",
    "\n",
    "cameras = data[\"camera\"]\n",
    "depths =  data[\"depth\"]\n",
    "alphas = data[\"alpha\"]\n",
    "rgbs= data[\"rgb\"]\n",
    "\n",
    "Hs, Ws, intrinsics, c2ws = extract_camera_params(cameras)\n",
    "W, H = int(Ws[0].item()), int(Hs[0].item())\n",
    "assert (depths.shape == alphas.shape)\n",
    "coords = []\n",
    "rgbas = []\n",
    "\n",
    "#test for first image\n",
    "intrinsic = intrinsics[0][:3, :3]   # shape: (3, 3)\n",
    "c2w   = c2ws[0]                     # shape: (4, 4)\n",
    "depth = depths[0]                   # shape: (H, W) \n",
    "alpha = alphas[0]                   # shape: (H, W)    \n",
    "rgba = rgbs[0]                      # (H, W, 3)\n",
    "\n",
    "for idx, h, w , intrinsic, c2w, depth, alpha in enumerate(zip(Hs, Ws, intrinsics, c2ws, depths, alphas)):\n",
    "\n",
    "    if idx >= 10:\n",
    "        break\n",
    "    \n",
    "    # 2) Create a grid of pixel coordinates in homogeneous form: (3, H*W)\n",
    "    #    Here, we flatten them for easier matrix multiplication.\n",
    "    i_coords = torch.arange(W, device=device)  # 0..W-1\n",
    "    j_coords = torch.arange(H, device=device)  # 0..H-1\n",
    "    i_grid, j_grid = torch.meshgrid(i_coords, j_coords, indexing=\"xy\")  # shape (W, H) each\n",
    "\n",
    "    # Flatten to (W*H,)\n",
    "    i_flat = i_grid.flatten()\n",
    "    j_flat = j_grid.flatten()\n",
    "    pix_coords = torch.stack([i_flat, j_flat, torch.ones_like(i_flat)], dim=0)  # shape: (3, H*W)\n",
    "\n",
    "    # 3) Multiply by the inverse of intrinsics to get camera directions (unnormalized).\n",
    "    K_inv = torch.linalg.inv(intrinsic)         # shape: (3, 3)\n",
    "    pix_coords = pix_coords.to(dtype=torch.float32)  # Ensure dtype is torch.float32\n",
    "    # print(f'type of K_inv : {K_inv.dtype}')\n",
    "    # print(f'type of pix_coords : {pix_coords.dtype}')\n",
    "    cam_dirs = K_inv @ pix_coords               # shape (3, W*H)\n",
    "\n",
    "    # 4) Multiply each ray direction by the corresponding depth to get actual camera-frame 3D coords.\n",
    "    depth_flat = depth.flatten()                # shape (W*H,)\n",
    "    cam_points_3D = cam_dirs * depth_flat       # shape (3, W*H)\n",
    "\n",
    "    # 5) Convert to homogeneous camera coordinates: (4, W*H)\n",
    "    ones = torch.ones(1, cam_points_3D.shape[1], device=device)\n",
    "    cam_points_hom = torch.cat([cam_points_3D, ones], dim=0)  # shape (4, W*H)\n",
    "\n",
    "    # 6) Transform these camera points into world coordinates using c2w (camera->world).\n",
    "    world_points_hom = c2w @ cam_points_hom  # shape (4, W*H) do i need to inverse it ? \n",
    "\n",
    "    # 7) Divide by the last row (perspective division) to get 3D points in world coords.\n",
    "    # world_points_3D = world_points_hom[:3]\n",
    "    world_points_3D = world_points_hom[:3] / world_points_hom[3].unsqueeze(0)  # shape (3, W*H)\n",
    "\n",
    "    # 8) Reshape to (H, W, 3) so it matches the image layout.\n",
    "    rays_d = world_points_3D.permute(1, 0).reshape(H, W, 3)  # shape (H, W, 3)\n",
    "\n",
    "    # 9) get rays center\n",
    "    rays_o = np.broadcast_to(c2w[:3, 3], rays_d.shape)  # Shape: (H, W, 3)\n",
    "    rays_o = torch.tensor(rays_o, dtype=torch.float32).to(depth.device)\n",
    "\n",
    "    # pts = rays_o + rays_d * depths[0][..., np.newaxis]  # Shape: (H, W, 3)\n",
    "    pts = rays_o + rays_d\n",
    "    mask = alphas[0].bool()  # Shape: (H, W)\n",
    "    valid_pts = pts[mask].cpu().numpy()  # Extract only valid 3D points\n",
    "    coords.append(valid_pts)\n",
    "    # print(valid_pts)\n",
    "\n",
    "    channels = {}\n",
    "    if rgba is not None:\n",
    "        print(f'shape of rgba : {rgba.shape}')\n",
    "        valid_rgb = rgba[mask]  # Shape: (N, 3)\n",
    "        print(f'shape of valid_rgb : {valid_rgb.shape}')\n",
    "        # Store each channel separately\n",
    "        channels['R'] = valid_rgb[:, 0]\n",
    "        channels['G'] = valid_rgb[:, 1]\n",
    "        channels['B'] = valid_rgb[:, 2]\n",
    "\n",
    "\n",
    "    point_cloud = PointCloud(coords, channels)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "from torch_3dgs.point import PointCloud\n",
    "\n",
    "# Create a Plotly 3D scatter plot of the point cloud\n",
    "\n",
    "for id, coords, channel in enumerate(zip(point_cloud.coords, point_cloud.channels)):\n",
    "    print(f'shape of coords : {coords.shape}')\n",
    "    print(f'shape of channels : {channel.shape}')\n",
    "\n",
    "    coords_array = np.array(point_cloud.coords[id])\n",
    "    # Prepare color values for Plotly visualization. If channels exist, create color strings.\n",
    "    if channels:\n",
    "        colors = [\n",
    "            \"rgb({},{},{})\".format(int(r), int(g), int(b))\n",
    "            for r, g, b in zip(channels['R'], channels['G'], channels['B'])\n",
    "        ]\n",
    "    else:\n",
    "        colors = \"blue\"\n",
    "    print(coords_array.shape)\n",
    "    \n",
    "    # Flatten the coordinates array to 1D\n",
    "    coords_array = coords_array.reshape(-1, 3)\n",
    "    scatter = go.Scatter3d(\n",
    "        x=coords_array[:, 0],\n",
    "        y=coords_array[:, 1],\n",
    "        z=coords_array[:, 2],\n",
    "        mode='markers',\n",
    "        marker=dict(size=2, color=colors, opacity=0.8)\n",
    "    )\n",
    "\n",
    "    layout = go.Layout(\n",
    "        scene=dict(\n",
    "            xaxis_title='X',\n",
    "            yaxis_title='Y',\n",
    "            zaxis_title='Z'\n",
    "        ),\n",
    "        margin=dict(l=0, r=0, b=0, t=0)\n",
    "    )\n",
    "\n",
    "    fig = go.Figure(data=[scatter], layout=layout)\n",
    "    # fig.show()\n",
    "    fig.write_image(f\"point_cloud_{id}.png\", width=800, height=600)\n",
    "    #save point cloud\n",
    "    output_path = os.path.join(config.output_folder, f\"point_cloud_{id}.ply\")\n",
    "    point_cloud.save(output_path)\n",
    "    print(f\"Point cloud saved to {output_path}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ev_hw1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
